{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TFAutoModel, AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/dataset_csv/dataset_pidana_umum.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pilih fitur dan label\n",
    "features = df['riwayat_dakwaan']  # Teks yang digunakan untuk prediksi\n",
    "labels = df['sub_klasifikasi']    # Kategori pidana\n",
    "\n",
    "# Drop nilai kosong dan pastikan input valid\n",
    "df = df.dropna(subset=['riwayat_dakwaan', 'sub_klasifikasi'])\n",
    "df['riwayat_dakwaan'] = df['riwayat_dakwaan'].astype(str)\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    max_length=200,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "# Encode Label\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data (contoh manual split)\n",
    "train_encodings = encodings\n",
    "y_train = encoded_labels\n",
    "test_encodings = encodings\n",
    "y_test = encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"indobenchmark/indobert-base-p2\"\n",
    "pretrained_bert = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Custom layer untuk membungkus BERT\n",
    "class BERTLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BERTLayer, self).__init__(**kwargs)\n",
    "        self.bert = pretrained_bert\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # Ambil token CLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Functional API untuk Model\n",
    "input_ids = Input(shape=(200,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(200,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Gunakan BERT layer\n",
    "cls_token = BERTLayer()([input_ids, attention_mask])\n",
    "\n",
    "# Tambahkan dense layer untuk prediksi\n",
    "dense_layer = Dense(128, activation='relu')(cls_token)\n",
    "output = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "# Bangun model\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "history = model.fit(\n",
    "    x={\n",
    "        \"input_ids\": train_encodings['input_ids'],\n",
    "        \"attention_mask\": train_encodings['attention_mask']\n",
    "    },\n",
    "    y=y_train,\n",
    "    validation_data=(\n",
    "        {\n",
    "            \"input_ids\": test_encodings['input_ids'],\n",
    "            \"attention_mask\": test_encodings['attention_mask']\n",
    "        },\n",
    "        y_test\n",
    "    ),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(\n",
    "    x={\n",
    "        \"input_ids\": test_encodings['input_ids'],\n",
    "        \"attention_mask\": test_encodings['attention_mask']\n",
    "    },\n",
    "    y=y_test\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi pada Data Baru\n",
    "predictions = model.predict({\n",
    "    \"input_ids\": test_encodings['input_ids'][:5],\n",
    "    \"attention_mask\": test_encodings['attention_mask'][:5]\n",
    "})\n",
    "\n",
    "# Ambil label prediksi\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "print(\"Predicted Categories:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Menimbang, bahwa terdakwa...\"\n",
    "tokenized_example = tokenizer(\n",
    "    example_text,\n",
    "    max_length=200,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# Prediksi untuk contoh\n",
    "example_prediction = model.predict({\n",
    "    \"input_ids\": tokenized_example[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_example[\"attention_mask\"]\n",
    "})\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(example_prediction, axis=1))\n",
    "print(\"Predicted Category for Example:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer dan model\n",
    "model_name = \"indobenchmark/indobert-base-p2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_bert = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Custom Layer untuk BERT\n",
    "class BERTLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BERTLayer, self).__init__(**kwargs)\n",
    "        self.bert = pretrained_bert\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # Ambil token CLS\n",
    "\n",
    "# Preprocessing Dataset\n",
    "# Contoh data (gantikan dengan dataset Anda)\n",
    "texts = [\"Menimbang, bahwa terdakwa...\", \"Hakim memutuskan bahwa...\", \"Saksi memberikan keterangan...\"]\n",
    "labels = [\"hukuman\", \"putusan\", \"kesaksian\"]\n",
    "\n",
    "# Tokenisasi data\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    max_length=200,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split data (contoh manual split)\n",
    "train_encodings = encodings\n",
    "y_train = encoded_labels\n",
    "test_encodings = encodings\n",
    "y_test = encoded_labels\n",
    "\n",
    "# Model Building\n",
    "input_ids = Input(shape=(200,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = Input(shape=(200,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "cls_token = BERTLayer()([input_ids, attention_mask])\n",
    "dense_layer = Dense(128, activation='relu')(cls_token)\n",
    "output = Dense(len(label_encoder.classes_), activation='softmax')(dense_layer)\n",
    "\n",
    "model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "model.summary()\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    x={\n",
    "        \"input_ids\": train_encodings['input_ids'],\n",
    "        \"attention_mask\": train_encodings['attention_mask']\n",
    "    },\n",
    "    y=y_train,\n",
    "    validation_data=(\n",
    "        {\n",
    "            \"input_ids\": test_encodings['input_ids'],\n",
    "            \"attention_mask\": test_encodings['attention_mask']\n",
    "        },\n",
    "        y_test\n",
    "    ),\n",
    "    epochs=5,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Evaluasi Model\n",
    "loss, accuracy = model.evaluate(\n",
    "    x={\n",
    "        \"input_ids\": test_encodings['input_ids'],\n",
    "        \"attention_mask\": test_encodings['attention_mask']\n",
    "    },\n",
    "    y=y_test\n",
    ")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Prediksi pada Data Baru\n",
    "predictions = model.predict({\n",
    "    \"input_ids\": test_encodings['input_ids'][:5],\n",
    "    \"attention_mask\": test_encodings['attention_mask'][:5]\n",
    "})\n",
    "\n",
    "# Decode hasil prediksi\n",
    "predicted_classes = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "print(\"Predicted Categories:\", predicted_classes)\n",
    "\n",
    "# Prediksi untuk Kalimat Baru\n",
    "example_text = \"Menimbang, bahwa terdakwa...\"\n",
    "tokenized_example = tokenizer(\n",
    "    example_text,\n",
    "    max_length=200,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "example_prediction = model.predict({\n",
    "    \"input_ids\": tokenized_example[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_example[\"attention_mask\"]\n",
    "})\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(example_prediction, axis=1))\n",
    "print(\"Predicted Category for Example:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# **1. Load Dataset**\n",
    "dataset_path = '../outputtesting4.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# **2. Pastikan Kolom Data**\n",
    "print(data.head())\n",
    "texts = data['PASAL'].values\n",
    "labels = data['ISI_PASAL'].values\n",
    "\n",
    "# **3. Cek Jumlah Teks dan Labels**\n",
    "print(\"Jumlah Teks:\", len(texts))\n",
    "print(\"Jumlah Labels:\", len(labels))\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "# **4. Hapus Baris dengan Missing Values**\n",
    "data = data.dropna(subset=['PASAL', 'ISI_PASAL'])\n",
    "texts = data['PASAL'].values\n",
    "labels = data['ISI_PASAL'].values\n",
    "\n",
    "# **5. Encode Labels**\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# **6. Load IndoBERT and Tokenizer**\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "bert_model = BertModel.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "\n",
    "# **7. Tokenize and Prepare Dataset**\n",
    "max_length = 200\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(0),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, encoded_labels, test_size=0.2, random_state=42)\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, max_length)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer, max_length)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# **8. Define Model**\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = len(label_encoder.classes_)\n",
    "model = BertClassifier(bert_model, num_classes)\n",
    "\n",
    "# **9. Define Training Components**\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# **10. Training Function**\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "# **11. Evaluation Function**\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)\n",
    "\n",
    "# **12. Save Model**\n",
    "torch.save(model.state_dict(), \"indobert_classification_model.pth\")\n",
    "\n",
    "# **13. Test Prediction**\n",
    "def predict_text(model, tokenizer, text, label_encoder, device, max_length):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predicted_class_index = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "    return label_encoder.inverse_transform([predicted_class_index])[0]\n",
    "\n",
    "# Example Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"senjata\"\n",
    "predicted_class = predict_text(model, tokenizer, example_text, label_encoder, device, max_length)\n",
    "print(f\"Predicted Class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
